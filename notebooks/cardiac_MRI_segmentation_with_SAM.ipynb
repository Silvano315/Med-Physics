{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook project about testing SAM to segment Cardiac MRI scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Silvano315/Med-Physics.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory\n",
    "\n",
    "import os \n",
    "\n",
    "os.chdir(\"Med-Physics\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your Kaggle API to /root/.config/kaggle and /root/.kaggle/kaggle.json\n",
    "\n",
    "os.makedirs('/root/.kaggle', exist_ok = True)\n",
    "\n",
    "!cp /content/drive/MyDrive/Kaggle_api/kaggle.json /root/.config/kaggle.json\n",
    "!cp /content/drive/MyDrive/Kaggle_api/kaggle.json /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "\n",
    "!pip install segment_anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/silvanoquarto/Desktop/LAVORO/MEDICAL_PHYSICS/Med-Physics'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you're running this repository LOCALLY, RUN this cell:\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "\n",
    "import kaggle\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set up device to use\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset(dataset_name : str = None, kaggle_url : str = None):\n",
    "    \"\"\"\n",
    "    Download ACDC dataset from Kaggle using Kaggle API.\n",
    "    Requires:\n",
    "    1. Kaggle account\n",
    "    2. API token (kaggle.json) in ~/.kaggle/\n",
    "    3. kaggle package installed: pip install kaggle\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = Path('data')\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {dataset_name} dataset from Kaggle...\")\n",
    "    try:\n",
    "        kaggle.api.authenticate()\n",
    "        kaggle.api.dataset_download_files(\n",
    "            kaggle_url,\n",
    "            path=data_dir,\n",
    "            unzip=True\n",
    "        )\n",
    "        print(\"Dataset downloaded and extracted successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        print(\"\\nPlease ensure you have:\")\n",
    "        print(\"1. Created a Kaggle account\")\n",
    "        print(\"2. Generated an API token from https://www.kaggle.com/settings\")\n",
    "        print(\"3. Placed kaggle.json in ~/.kaggle/\")\n",
    "        print(\"4. Set appropriate permissions: chmod 600 ~/.kaggle/kaggle.json\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ACDC dataset from Kaggle...\n",
      "Dataset URL: https://www.kaggle.com/datasets/anhoangvo/acdc-dataset\n",
      "Dataset downloaded and extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download ACDC dataset from Kaggle using API key\n",
    "\n",
    "download_kaggle_dataset(dataset_name=\"ACDC\", kaggle_url='anhoangvo/acdc-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cardiac MRI Segmentation with SAM - Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SAM Model\n",
    "\n",
    "def setup_sam():\n",
    "    \"\"\"Initialize and load SAM model.\"\"\"\n",
    "\n",
    "    sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
    "    checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
    "\n",
    "    if not os.path.exists(sam_checkpoint):\n",
    "        print(\"Downloading SAM checkpoint...\")\n",
    "        urllib.request.urlretrieve(checkpoint_url, sam_checkpoint)\n",
    "\n",
    "    model_type = \"vit_b\"\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "    sam.to(device=DEVICE)\n",
    "\n",
    "    return sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SAM checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silvanoquarto/Desktop/LAVORO/MEDICAL_PHYSICS/Med-Physics/.venv/lib/python3.11/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "sam = setup_sam()\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_h5_files(data_dir: Path, subset: str = 'training'):\n",
    "    \"\"\"\n",
    "    List all H5 files in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Base directory containing the dataset\n",
    "        subset: 'training' or 'testing'\n",
    "        \n",
    "    Returns:\n",
    "        List of paths to H5 files\n",
    "    \"\"\"\n",
    "    if 'training' in subset:\n",
    "        pattern = f\"**/*_{subset}/*.h5\"\n",
    "    else:\n",
    "        pattern = f\"**/*_{subset}_*/*.h5\"\n",
    "    \n",
    "    return list(data_dir.glob(pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_volumes_files = list_h5_files(data_dir, 'training_volumes')\n",
    "training_slices_files = list_h5_files(data_dir, 'training_slices')\n",
    "testing_volumes_files = list_h5_files(data_dir, 'testing')\n",
    "\n",
    "print(f\"Found {len(training_slices_files)} training slises \\\n",
    "and {len(training_volumes_files)} training volumes files \\\n",
    "and {len(testing_volumes_files)} testing volumes files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_info(file_lists: Dict[str, List[Path]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame with information about the dataset files.\n",
    "    \n",
    "    Args:\n",
    "        file_lists: Dictionary with keys 'training_volumes', 'training_slices', 'testing_volumes'\n",
    "                   and corresponding lists of Path objects\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: patient_id, frame, slice (if applicable), type, path\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    pattern = r'patient(\\d+)_frame(\\d+)(?:_slice_(\\d+))?'\n",
    "    \n",
    "    for data_type, files in file_lists.items():\n",
    "        for file_path in files:\n",
    "            match = re.search(pattern, file_path.name)\n",
    "            if match:\n",
    "                patient_id = match.group(1)\n",
    "                frame = match.group(2)\n",
    "                slice_num = match.group(3)\n",
    "                \n",
    "                data_entry = {\n",
    "                    'patient_id': int(patient_id),\n",
    "                    'frame': int(frame),\n",
    "                    'slice': int(slice_num) if slice_num else None,\n",
    "                    'type': data_type,\n",
    "                    'path': str(file_path)\n",
    "                }\n",
    "                all_data.append(data_entry)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    df = df.sort_values(['patient_id', 'frame', 'slice'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lists = {\n",
    "        'training_volumes': training_volumes_files,\n",
    "        'training_slices': training_slices_files,\n",
    "        'testing_volumes': testing_volumes_files\n",
    "    }\n",
    "\n",
    "dataset_df = create_dataset_info(file_lists)\n",
    "print(\"\\nDataset Overview:\")\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total files: {len(dataset_df)}\")\n",
    "print(\"\\nFiles per type:\")\n",
    "print(dataset_df['type'].value_counts())\n",
    "print(\"\\nUnique patients:\", dataset_df['patient_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_data(file_path: str):\n",
    "    \"\"\"\n",
    "    Load data from H5 file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to H5 file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing image and mask data\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        print(f\"Available keys in {Path(file_path).name}:\", list(f.keys()))\n",
    "        \n",
    "        data = {}\n",
    "        if 'image' in f:\n",
    "            data['image'] = f['image'][:]\n",
    "        if 'label' in f:\n",
    "            data['label'] = f['label'][:]\n",
    "        if 'scribble' in f:\n",
    "            data['scribble'] = f['scribble'][:]\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_volumes_files:\n",
    "        sample_data = load_h5_data(training_volumes_files[0])\n",
    "\n",
    "sample_data['image'], sample_data['label'], sample_data['scribble']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
